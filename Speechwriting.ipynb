{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOI8lacOoFXss/PAuZXTfNg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mattrfu/speechwriting-LLM/blob/main/Speechwriting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDM-AM_VhKT1"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[sentencepiece]\n",
        "!pip install datasets\n",
        "!pip install accelerate\n",
        "!pip install trl\n",
        "!pip install jsonlines\n",
        "!pip install einops\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "SfUd3uVkjR9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoTokenizer\n",
        "# import transformers\n",
        "# import torch\n",
        "\n",
        "# model = \"meta-llama/Llama-2-7b-hf\"\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "# pipeline = transformers.pipeline(\n",
        "#     \"text-generation\",\n",
        "#     model=model,\n",
        "#     torch_dtype=torch.float16,\n",
        "#     device_map=\"auto\",\n",
        "# )\n",
        "\n",
        "# text = \"Write a speech in the style of David Hogg about waffles\"\n",
        "# sequences = pipeline(\n",
        "#     text,\n",
        "#     do_sample=True,\n",
        "#     top_k=10,\n",
        "#     num_return_sequences=1,\n",
        "#     eos_token_id=tokenizer.eos_token_id,\n",
        "#     max_length=1000,\n",
        "# )\n",
        "# for seq in sequences:\n",
        "#     print(f\"Result: {seq['generated_text']}\")\n"
      ],
      "metadata": {
        "id": "OgTeXT7XiLiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", device_map=\"auto\")"
      ],
      "metadata": {
        "id": "3p5JmnvMQ45X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !export 'PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:16'"
      ],
      "metadata": {
        "id": "2uYlkV-DTFYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text = \"Write a speech in the style of David Hogg about waffles\"\n",
        "# inputs = tokenizer(text, return_tensors = \"pt\")\n",
        "# outputs = model(**inputs)\n",
        "# print(inputs)\n",
        "# print(outputs)"
      ],
      "metadata": {
        "id": "QmN2byjOI_mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !curl https://huggingface.co/datasets/clam004/antihallucination_dataset/resolve/main/antihallucination.jsonl -o antiha llucination.jsonl\n"
      ],
      "metadata": {
        "id": "i2S1NbkOPlm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs"
      ],
      "metadata": {
        "id": "LD6H9OPDhBsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs[\"input_ids\"].shape"
      ],
      "metadata": {
        "id": "LFK2eLNThFRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoTokenizer\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")"
      ],
      "metadata": {
        "id": "AZ3Ok_Jqglik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text = \"<Prompt>: Write a short length speech for a national audience in the style of David Hogg on the topic of gun control. <Bot>:First off, I’m gonna start off by putting this price tag right here as a reminder for you guys to know how much Marco Rubio took for every student’s life in Florida. One dollar and five cents. The cold grasp of corruption shackles the District of Columbia. The winter is over. Change is here. The sun shines on a new day, and the day is ours. First-time voters show up 18 percent of the time at midterm elections. Not anymore. Now, who here is gonna vote in the 2018 election? If you listen real close, you can hear the people in power shaking. They’ve gotten used to being protective of their position, the safety of inaction. Inaction is no longer safe. And to that, we say: No more. Ninety-six people die every day from guns in our country, yet most representatives have no public stance on guns. And to that, we say: No more. We are going to make this the voting issue. We are going to take this to every election, to every state, in every city. We are going to make sure the best people get in our elections to run, not as politicians, but as Americans. Because this — this is not cutting it. When people try to suppress your vote, and there are people who stand against you because you’re too young, we say: No more. When politicians say that your voice doesn’t matter because the NRA owns them, we say: No more. When politicians send their thoughts and prayers with no action, we say: No more. And to those politicians supported by the NRA, that allow the continued slaughter of our children and our future, I say: Get your résumés ready. Today is the beginning of spring, and tomorrow is the beginning of democracy. Now is the time to come together, not as Democrats, not as Republicans, but as Americans. Americans of the same flesh and blood, that care about one thing and one thing only, and that’s the future of this country and the children that are going to lead it. Now, they will try to separate us in demographics. They will try to separate us by religion, race, congressional district and class. They will fail. We will come together. We will get rid of these public servants that only serve the gun lobby, and we will save lives. You are those heroes. Lastly, let’s put the USA over the NRA. This is the start of the spring and the blossoming of our democracy. So let’s take this to our local legislators, and let’s take this to midterm elections, because without the persistence — heat — without the persistence of voters and Americans everywhere, getting out to every election, democracy will not flourish. But it can, and it will. So, I say to those politicians that say change will not come, I say: We will not stop until every man, every woman, every child, and every American can live without fear of gun violence. And to that, I say: No more. Thank you, I love you all, God bless all of you, and God bless America. We can, and we will, change the world.\"\n",
        "# inputs = tokenizer(text, return_tensors = \"pt\")"
      ],
      "metadata": {
        "id": "MJb8apydg4b-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True)"
      ],
      "metadata": {
        "id": "ZHXodkJ8kID-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoTokenizer\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\")"
      ],
      "metadata": {
        "id": "GFwg5Fjhki0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zR8meV9zNqwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !accelerate estimate-memory mistralai/Mistral-7B-v0.1"
      ],
      "metadata": {
        "id": "UHv7wXvH5J0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !accelerate estimate-memory stabilityai/stablelm-3b-4e1t"
      ],
      "metadata": {
        "id": "h4Fdgyvc5w8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !accelerate estimate-memory meta-llama/Llama-2-7b-hf"
      ],
      "metadata": {
        "id": "1eWb2Osy51hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "MThwWzAf9d2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer, AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"GreenBitAI/LLaMA-2-1.1B-2bit-groupsize32\", device_map=device, torch_dtype=torch.float16, trust_remote_code=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"GreenBitAI/LLaMA-2-1.1B-2bit-groupsize32\")"
      ],
      "metadata": {
        "id": "TpRyT4OF2TNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jsonlines\n",
        "\n",
        "with jsonlines.open('output.jsonl', 'w') as writer:\n",
        "    writer.write_all(dataset)"
      ],
      "metadata": {
        "id": "5jflEp0CPnnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files=\"output.jsonl\")\n",
        "dataset[\"train\"].shape"
      ],
      "metadata": {
        "id": "5z4rfAo2OtI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "def preprocess_function(example):\n",
        "    return tokenizer(example[\"text\"], truncation = True)\n",
        "\n",
        "tokenized_dataset = dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        ")\n",
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "AO04VEkOOOQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenized_dataset[\"train\"][0][\"input_ids\"])"
      ],
      "metadata": {
        "id": "ABTOV7ikS93f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ],
      "metadata": {
        "id": "V8GnMAGeIdfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    fp16=True,\n",
        "    output_dir=\"green_speech_model\",\n",
        "    evaluation_strategy=\"no\",\n",
        "    # evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=True,\n",
        "    do_eval=False, # Should delete\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "TEpwqY2hk1h1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "0j8nwjJ7V2nF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"<Prompt>: Write a short length speech for a national audience in the style of David Hogg on the topic of waffles. <Bot>:\"\n",
        "inputs = tokenizer(text, return_tensors = \"pt\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=300, do_sample=True, top_k=50, top_p=0.95)\n",
        "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "SA_CcayOSeMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"<Prompt>: Write a short length speech for a national audience in the style of David Hogg on the topic of waffles. <Bot>:\"\n",
        "new_model = AutoModelForCausalLM.from_pretrained(\"new_model\")\n",
        "inputs = tokenizer(text, return_tensors = \"pt\")\n",
        "outputs = new_model.generate(**inputs, max_new_tokens=300, do_sample=True, top_k=50, top_p=0.95)\n",
        "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "XjmwOBvbT4F3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}